import boto3
import os

# D√©tection de l'environnement (Docker/Airflow ou ex√©cution locale)
if os.getenv("AIRFLOW_HOME"):
    S3_ENDPOINT_URL = "http://localstack:4566"  # Utilis√© dans Docker
else:
    S3_ENDPOINT_URL = "http://localhost:4566"  # Utilis√© en local

S3_BUCKET_RAW = "raw"
LOCAL_FILE_PATH = "/opt/airflow/dags/data/raw/2022-2023_Football_Player_Stats.csv"
S3_OBJECT_NAME = "football_stats_raw.csv"

# Cr√©ation du client S3 avec des credentials fictifs
s3 = boto3.client(
    "s3",
    endpoint_url=S3_ENDPOINT_URL,
    aws_access_key_id="test",   # Valeur fictive (obligatoire pour LocalStack)
    aws_secret_access_key="test" # Valeur fictive (obligatoire pour LocalStack)
)

def create_bucket_if_not_exists(bucket_name):
    """V√©rifie si le bucket existe, sinon le cr√©e."""
    try:
        existing_buckets = [bucket['Name'] for bucket in s3.list_buckets().get('Buckets', [])]
        if bucket_name not in existing_buckets:
            print(f"üìå Bucket '{bucket_name}' introuvable. Cr√©ation en cours...")
            s3.create_bucket(Bucket=bucket_name)
            print(f"‚úÖ Bucket '{bucket_name}' cr√©√© avec succ√®s.")
        else:
            print(f"‚úÖ Bucket '{bucket_name}' d√©j√† existant.")
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification/cr√©ation du bucket : {e}")

def upload_to_s3():
    """Charge le fichier CSV dans le bucket S3 (LocalStack)"""
    if not os.path.exists(LOCAL_FILE_PATH):
        print(f"‚ùå Erreur : Fichier {LOCAL_FILE_PATH} non trouv√©.")
        return

    # V√©rifier et cr√©er le bucket si n√©cessaire
    create_bucket_if_not_exists(S3_BUCKET_RAW)

    # Envoi du fichier vers S3
    try:
        s3.upload_file(LOCAL_FILE_PATH, S3_BUCKET_RAW, S3_OBJECT_NAME)
        print(f"‚úÖ Fichier {LOCAL_FILE_PATH} charg√© sur S3 dans {S3_BUCKET_RAW}/{S3_OBJECT_NAME}")
    except Exception as e:
        print(f"‚ùå Erreur lors de l'upload du fichier : {e}")

if __name__ == "__main__":
    upload_to_s3()
